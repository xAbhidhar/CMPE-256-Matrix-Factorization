{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Collaborative Filtering ALS Recommender System using Spark MLlib adapted from\n",
    "the Spark Summit 2014 Recommender System training example.\n",
    "\n",
    "Developed By: Pranav Masariya\n",
    "Supervisor: Dr. Magdalini Eirinaki\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.ml.recommendation import ALS as mlals\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import math\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calling spark session to register application\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Recom\") \\\n",
    "    .config(\"spark.recom.demo\", \"1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading and Parsing Dataset\n",
    "    Each line in the ratings dataset (ratings.csv) is formatted as:\n",
    "         userId,movieId,rating,timestamp\n",
    "    Each line in the movies (movies.csv) dataset is formatted as:\n",
    "        movieId,title,genres\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "# Load ratings\n",
    "ratings_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+\n",
      "|userId|movieId|      rating|\n",
      "+------+-------+------------+\n",
      "|   149|      2| 0.927561837|\n",
      "|   149|      3|-0.839222615|\n",
      "|   149|      5|-0.839222615|\n",
      "+------+-------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For the simplicity of this tutorial\n",
    "    For each line in the ratings dataset, we create a tuple of (UserID, MovieID, Rating). \n",
    "    We drop the timestamp because we do not need it for this recommender.\n",
    "\"\"\"\n",
    "\n",
    "#ratings_df = ratings_df.drop('timestamp')\n",
    "ratings_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to determine the best ALS parameters, we will use the small dataset. \n",
    "We need first to split it into train, validation, and test datasets.\n",
    "\"\"\"\n",
    "(trainingData,validationData,testData) = ratings_df.randomSplit([0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare test and validation set. They should not have ratings\n",
    "\n",
    "validation_for_predict = validationData.select('userId','movieId')\n",
    "test_for_predict = testData.select('userId','movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spark MLlib library for Machine Learning provides a Collaborative Filtering implementation by \n",
    "using Alternating Least Squares. The implementation in MLlib has the following parameters:\n",
    "\n",
    "    1. numBlocks is the number of blocks used to parallelize computation (set to -1 to auto-configure).\n",
    "    2. rank is the number of latent factors in the model.\n",
    "    3. iterations is the number of iterations to run.\n",
    "    4. lambda specifies the regularization parameter in ALS.\n",
    "    5. implicitPrefs specifies whether to use the explicit \n",
    "        feedback ALS variant or one adapted for implicit feedback data.\n",
    "    6. alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline \n",
    "        confidence in preference observations.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "seed = 5 #Random seed for initial matrix factorization model. A value of None will use system time as the seed.\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1 #run for different lambdas - e.g. 0.01\n",
    "ranks = [4, 8, 12] #number of features\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 1.0990339415\n",
      "For rank 8 the RMSE is 0.965288738897\n",
      "For rank 12 the RMSE is 0.927297201283\n",
      "The best model was trained with rank 12\n"
     ]
    }
   ],
   "source": [
    "# Let us traing our dataset and check the best rank with lowest RMSE\n",
    "# predictAll method of the ALS takes only RDD format and hence we need to convert our dataframe into RDD\n",
    "# df.rdd will automatically converts Dataframe into RDD\n",
    "\n",
    "for rank in ranks:\n",
    "    model = ALS.train(trainingData, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(validation_for_predict.rdd).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = validationData.rdd.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()) # RMSE Error\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print ('For rank %s the RMSE is %s' % (rank, error))\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print ('The best model was trained with rank %s' % best_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 1.07152402993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe best part is we do not have to worry about RDD any more with this library\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark will soon deprecate MLLIb package. \n",
    "They are focusing more on ML packages with standard machine learning implementation\n",
    "Let's see that package also\n",
    "\"\"\"\n",
    "als =  mlals(maxIter=iterations,rank=4,seed=seed,regParam=regularization_parameter, userCol=\"userId\", itemCol=\"movieId\",ratingCol=\"rating\")\n",
    "modelML = als.fit(trainingData)\n",
    "pred = modelML.transform(validationData)\n",
    "pred = pred.where(pred['prediction'] != 'NaN')\n",
    "    \n",
    "# Evaluate the model by computing RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(pred)\n",
    "\n",
    "print ('RMSE is %s' % rmse)\n",
    "\n",
    "\"\"\"\n",
    "The best part is we do not have to worry about RDD any more with this library\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's take test dataset and get ratings\n",
    "predictions_test = model.predictAll(test_for_predict.rdd).map(lambda r: ((r[0], r[1]), r[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((460, 5), 0.14467233353957915),\n",
       " ((460, 14), -0.18127298287070573),\n",
       " ((912, 16), -0.8881264110490364)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## visualize preditions, here third element is predictions generated by ALS Model\n",
    "predictions_test.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's start recommending movies.\n",
    "I have written a method to call recommendations for a perticular user from test data\n",
    "\n",
    "TODO: You need to execute one more step before calling getRecommendations, \n",
    "      Think about that step. If you go through the seps below, you will realize it soon.\n",
    "\"\"\"\n",
    "def getRecommendations(user,testDf,trainDf,model):\n",
    "    # get all user and his/her rated movies\n",
    "    userDf = testDf.filter(testDf.userId == user)\n",
    "    # filter movies from main set which have not been rated by selected user\n",
    "    # and pass it to model we sreated above\n",
    "    mov = trainDf.select('movieId').subtract(userDf.select('movieId'))\n",
    "    \n",
    "    # Again we need to covert our dataframe into RDD\n",
    "    pred_rat = model.predictAll(mov.rdd.map(lambda x: (user, x[0]))).collect()\n",
    "    \n",
    "    # Get the top recommendations\n",
    "    recommendations = sorted(pred_rat, key=lambda x: x[2], reverse=True)[:50]\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies recommended for:969\n"
     ]
    }
   ],
   "source": [
    "# Assign user id for which we need recommendations#user = 337\n",
    "user = 969\n",
    "\n",
    "# Call getRecommendations method\n",
    "derived_rec = getRecommendations(user,testData,trainingData,model)\n",
    "\n",
    "print (\"Movies recommended for:%d\" % user)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=969, product=2, rating=0.028894738022447047),\n",
       " Rating(user=969, product=20, rating=0.01744645490047858),\n",
       " Rating(user=969, product=12, rating=-0.012485262634038707),\n",
       " Rating(user=969, product=6, rating=-0.024314851056819187),\n",
       " Rating(user=969, product=18, rating=-0.17090410571053893),\n",
       " Rating(user=969, product=8, rating=-0.3109935539096452),\n",
       " Rating(user=969, product=7, rating=-0.4136548171532802),\n",
       " Rating(user=969, product=1, rating=-0.45489023824419766),\n",
       " Rating(user=969, product=19, rating=-0.4800760295483392),\n",
       " Rating(user=969, product=13, rating=-0.6949371250882268),\n",
       " Rating(user=969, product=4, rating=-0.7103406776721104),\n",
       " Rating(user=969, product=10, rating=-0.8980401526672288),\n",
       " Rating(user=969, product=3, rating=-0.9277905747328498),\n",
       " Rating(user=969, product=9, rating=-1.0301495058540413),\n",
       " Rating(user=969, product=21, rating=-1.1122173966532916),\n",
       " Rating(user=969, product=11, rating=-2.0662874865403187),\n",
       " Rating(user=969, product=15, rating=-2.10287766524094),\n",
       " Rating(user=969, product=17, rating=-2.2000183157692947)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derived_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(user=969, product=2, rating=0.028894738022447047)\n",
      "Rating(user=969, product=20, rating=0.01744645490047858)\n",
      "Rating(user=969, product=12, rating=-0.012485262634038707)\n",
      "Rating(user=969, product=6, rating=-0.024314851056819187)\n",
      "Rating(user=969, product=18, rating=-0.17090410571053893)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print (derived_rec[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
